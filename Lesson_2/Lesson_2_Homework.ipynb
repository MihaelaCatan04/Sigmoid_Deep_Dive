{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MihaelaCatan04/Sigmoid_Deep_Dive/blob/main/Lesson_2/Lesson_2_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmfg_-nHBSfb"
      },
      "source": [
        "### Homework Nr. 2\n",
        "#### Tasks:\n",
        "1. Complete week 1 and 2 of second course from specialization;\n",
        "2. Complete the practical Task\n",
        "3. Watch the following videos, and try to integrate W&B in your code:\n",
        "    https://www.youtube.com/watch?v=tHAFujRhZLA&t=207s\n",
        "    \n",
        "#### Practical Task:\n",
        "Implement Neural Network using PyTorch to solve a task from selected Dataset.\n",
        "Dataset can be one of the\n",
        "1. https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
        "2. https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data\n",
        "3. Or other dataset that you can find interesting to solve with an NN\n",
        "\n",
        "Steps:\n",
        "1. Implement all needed libraries\n",
        "2. Load and display few images from the dataset\n",
        "3. Train-test split\n",
        "4. Create DataLoaders for each set\n",
        "5. Create Neural Network Class (multilayer)\n",
        "6. Create Training and Testing loop\n",
        "7. Train your model\n",
        "8. Test your model\n",
        "9. Output your results (accuracy and prediction examples)\n",
        "10. IMPORTANT! Repeat this process with different: Activation functions, gradient descent techniques (SGD, Mini-batch GD), different hyperparameters (epocs, learning rate, ......)\n",
        "11. Track your experiments, and show what was the combination that performed best, can you find the explanation for that?\n",
        "11. Draw conclusions and see what are the differences\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "9begAPxSDrKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to google drive to save the dataset there."
      ],
      "metadata": {
        "id": "5FkVVJPZECIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuDaglziqkjw",
        "outputId": "a559ed34-bbf3-46c4-8da7-f889ab3b0cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Kaggle to download the datasete."
      ],
      "metadata": {
        "id": "RXUYcl8_EJhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp drive/MyDrive/kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "XLbPb-9yqK3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download the dataset\n",
        "!kaggle datasets download -d johnsmith88/heart-disease-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4InBlrYqg-z",
        "outputId": "50d93cea-0821-409e-9aed-11316495a542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
            "License(s): unknown\n",
            "Downloading heart-disease-dataset.zip to /content\n",
            "  0% 0.00/6.18k [00:00<?, ?B/s]\n",
            "100% 6.18k/6.18k [00:00<00:00, 12.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip dataset\n",
        "!unzip heart-disease-dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEQRk-GSq1YW",
        "outputId": "48aa4cc7-43bd-479c-c185-d01fa7a9552f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  heart-disease-dataset.zip\n",
            "  inflating: heart.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install wandb as part of the homework."
      ],
      "metadata": {
        "id": "DcxFCTCXEO0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MIOZ21ptQBE",
        "outputId": "66c4a64f-0739-4a08-f18f-5b5206f9a3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login to wandb."
      ],
      "metadata": {
        "id": "TovkIrFYETtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7dFZCPTusbi",
        "outputId": "6f3a2e08-f188-4ba5-9d6b-b0edeeca7e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries."
      ],
      "metadata": {
        "id": "IT_GaVFYEXay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import wandb"
      ],
      "metadata": {
        "id": "IWDKmU6hwQqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset from kaggle and print its head using pandas."
      ],
      "metadata": {
        "id": "J7aCA99zEcEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv(\"heart.csv\")\n",
        "\n",
        "# Display data\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "kSzmVTr-wS23",
        "outputId": "31206704-bc58-4626-a4b6-a5630d891dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
              "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
              "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
              "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
              "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   2     3       0  \n",
              "1   0     3       0  \n",
              "2   0     3       0  \n",
              "3   1     3       0  \n",
              "4   3     2       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ac9bd5a-29e3-45ec-a9d1-8f403266756e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>212</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>168</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>203</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "      <td>3.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>145</td>\n",
              "      <td>174</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>148</td>\n",
              "      <td>203</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>161</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>294</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ac9bd5a-29e3-45ec-a9d1-8f403266756e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ac9bd5a-29e3-45ec-a9d1-8f403266756e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ac9bd5a-29e3-45ec-a9d1-8f403266756e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8ff7b04a-51e0-4f25-885f-121533c4b99d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ff7b04a-51e0-4f25-885f-121533c4b99d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8ff7b04a-51e0-4f25-885f-121533c4b99d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1025,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          65,\n          50,\n          54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 94,\n        \"max\": 200,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          128,\n          172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51,\n        \"min\": 126,\n        \"max\": 564,\n        \"num_unique_values\": 152,\n        \"samples\": [\n          267,\n          262\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23,\n        \"min\": 71,\n        \"max\": 202,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          180,\n          152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.175053255150173,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          2.8,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into train and test sets and normalize them because Neural Networks work only with noralized data."
      ],
      "metadata": {
        "id": "XQ7t__VREtK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "X = data.drop(\"target\", axis=1).values\n",
        "y = data[\"target\"].values\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "8X0vY0fjwaYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a HeartDataset class that will transform dataframes in dataloaders."
      ],
      "metadata": {
        "id": "2mwYrU_9E3NC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HeartDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "shOKalZyweiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaders\n",
        "train_dataset = HeartDataset(X_train, y_train)\n",
        "test_dataset = HeartDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "Xq0ZQvwPwiHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of dataloaders\n",
        "for X, y in test_loader:\n",
        "  print(f\"Shape of X: {X.shape}\")\n",
        "  print(f\"Shape of y: {y.shape}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctCGsYz4wtwB",
        "outputId": "0858598f-aaf7-4393-9f8d-4e43727ba4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: torch.Size([32, 13])\n",
            "Shape of y: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set device to CUDA if available."
      ],
      "metadata": {
        "id": "YpDGRhCkFI-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Your laptop is using {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJRnGmomwzpT",
        "outputId": "87c13fbb-754d-4106-f6d7-6c8f20ac5553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your laptop is using cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Neural Network with:\n",
        "\n",
        "*   Input layer: 13 neurons\n",
        "*   1st layer: 128 neurons, activation function - tanh\n",
        "*   2nd layer: 256 neurons, activation function - tanh\n",
        "*   3rd layer: 512 neurons, activation function - tanh\n",
        "*   Output layer: 1 neuron, activation function - Sigmoid\n",
        "\n"
      ],
      "metadata": {
        "id": "7upptp9BFMUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkTanhSigmoid(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_tanh_sigmoid_stack = nn.Sequential(\n",
        "            nn.Linear(13, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(2048, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        predictions = self.linear_tanh_sigmoid_stack(x)\n",
        "        return predictions\n",
        "\n",
        "model_tanh_sigmoid = NeuralNetworkTanhSigmoid().to(device)\n",
        "print(model_tanh_sigmoid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcih5lFmxgTF",
        "outputId": "ba7635d8-b0d2-4b40-c24a-faa1eb5badd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetworkTanhSigmoid(\n",
            "  (linear_tanh_sigmoid_stack): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=128, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (5): Tanh()\n",
            "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (7): Tanh()\n",
            "    (8): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "    (9): Tanh()\n",
            "    (10): Linear(in_features=2048, out_features=1, bias=True)\n",
            "    (11): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Neural Network with:\n",
        "\n",
        "*   Input layer: 13 neurons\n",
        "*   1st layer: 128 neurons, activation function - Sigmoid\n",
        "*   2nd layer: 256 neurons, activation function - Sigmoid\n",
        "*   3rd layer: 512 neurons, activation function - Sigmoid\n",
        "*   Output layer: 1 neuron, activation function - Sigmoid\n"
      ],
      "metadata": {
        "id": "j08gsnuEFpLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkSigmoid(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_sigmoid_stack = nn.Sequential(\n",
        "            nn.Linear(13, 128),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(2048, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        predictions = self.linear_sigmoid_stack(x)\n",
        "        return predictions\n",
        "\n",
        "model_sigmoid = NeuralNetworkSigmoid().to(device)\n",
        "print(model_sigmoid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBlfjw3gzAld",
        "outputId": "50079b15-ae69-4f55-bb4a-642465e296f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetworkSigmoid(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_sigmoid_stack): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=128, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): Sigmoid()\n",
            "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (5): Sigmoid()\n",
            "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (7): Sigmoid()\n",
            "    (8): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "    (9): Sigmoid()\n",
            "    (10): Linear(in_features=2048, out_features=1, bias=True)\n",
            "    (11): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Neural Network with:\n",
        "\n",
        "*   Input layer: 13 neurons\n",
        "*   1st layer: 128 neurons, activation function - ReLU\n",
        "*   2nd layer: 256 neurons, activation function - ReLU\n",
        "*   3rd layer: 512 neurons, activation function - ReLU\n",
        "*   Output layer: 1 neuron, activation function - Sigmoid\n",
        "\n"
      ],
      "metadata": {
        "id": "6RGhrCFEFzcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkReLUSigmoid(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_sigmoid_stack = nn.Sequential(\n",
        "            nn.Linear(13, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        predictions = self.linear_relu_sigmoid_stack(x)\n",
        "        return predictions\n",
        "\n",
        "model_relu_sigmoid = NeuralNetworkReLUSigmoid().to(device)\n",
        "print(model_relu_sigmoid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-qdTri-zIH3",
        "outputId": "a418673d-172b-4e82-e2c2-e95dde9af1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetworkReLUSigmoid(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_sigmoid_stack): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "    (9): ReLU()\n",
            "    (10): Linear(in_features=2048, out_features=1, bias=True)\n",
            "    (11): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the train function."
      ],
      "metadata": {
        "id": "YgwNB1fcF6Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_no, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.unsqueeze(1).to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_no % 5 == 0 or batch_no == num_batches - 1:\n",
        "            current = min((batch_no + 1) * len(X), size)\n",
        "            print(f\"Batch {batch_no + 1}/{num_batches} - Loss: {loss.item():.4f} [{current}/{size}]\")\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch Training Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "XO5SZlUlzckg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the test function."
      ],
      "metadata": {
        "id": "Ld_KavbHF-lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn, epoch=None, log_to_wandb=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.unsqueeze(1).to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += ((pred > 0.5) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    accuracy = correct / size\n",
        "\n",
        "    if log_to_wandb and epoch is not None:\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"test_loss\": test_loss,\n",
        "            \"test_accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "    print(f\"Test Results:\\n Accuracy: {(100 * accuracy):>0.1f}%, Avg Loss: {test_loss:.4f}\\n\")"
      ],
      "metadata": {
        "id": "8xBUlsW_zssU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a wandb run and save the configurations."
      ],
      "metadata": {
        "id": "yzkmE36EGBlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"neural-networks-homework\",\n",
        "    config={\n",
        "        \"dataset\": \"heart-disease-dataset\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": 32,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"optimizer\": \"Adam\"\n",
        "    },\n",
        "    settings=wandb.Settings(init_timeout=120)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "Y6OT1TQ3zvPO",
        "outputId": "b9e28da6-db9d-4645-ca80-57af2215022b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmihaelacatan\u001b[0m (\u001b[33mmihaela-catan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241122_044045-u8jaqks4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mihaela-catan/neural-networks-homework/runs/u8jaqks4' target=\"_blank\">pious-night-1</a></strong> to <a href='https://wandb.ai/mihaela-catan/neural-networks-homework' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mihaela-catan/neural-networks-homework' target=\"_blank\">https://wandb.ai/mihaela-catan/neural-networks-homework</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mihaela-catan/neural-networks-homework/runs/u8jaqks4' target=\"_blank\">https://wandb.ai/mihaela-catan/neural-networks-homework/runs/u8jaqks4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mihaela-catan/neural-networks-homework/runs/u8jaqks4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78964dfaba90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the 1st Neural Network:"
      ],
      "metadata": {
        "id": "iHOPaEYyGNra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetworkTanhSigmoid().to(device)\n",
        "loss_func = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_tanh_sigmoid.parameters(), lr=wandb.config.learning_rate)\n",
        "\n",
        "for epoch in range(wandb.config.epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------------------\")\n",
        "    train(train_loader, model, loss_func, optimizer)\n",
        "    test(test_loader, model, loss_func)\n",
        "\n",
        "torch.save(model.state_dict(), \"NeuralNetworkTanhSigmoid.pth\")\n",
        "wandb.save(\"NeuralNetworkTanhSigmoid.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sikt7OwO0UZy",
        "outputId": "3208fb65-0250-4ee6-f204-8a2dd52cdcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7004 [32/820]\n",
            "Batch 6/26 - Loss: 0.6963 [192/820]\n",
            "Batch 11/26 - Loss: 0.6993 [352/820]\n",
            "Batch 16/26 - Loss: 0.6968 [512/820]\n",
            "Batch 21/26 - Loss: 0.6978 [672/820]\n",
            "Batch 26/26 - Loss: 0.7036 [520/820]\n",
            "Epoch Training Loss: 0.6983\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 2\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6926 [32/820]\n",
            "Batch 6/26 - Loss: 0.7008 [192/820]\n",
            "Batch 11/26 - Loss: 0.6977 [352/820]\n",
            "Batch 16/26 - Loss: 0.7008 [512/820]\n",
            "Batch 21/26 - Loss: 0.7037 [672/820]\n",
            "Batch 26/26 - Loss: 0.6979 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 3\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7028 [32/820]\n",
            "Batch 6/26 - Loss: 0.6945 [192/820]\n",
            "Batch 11/26 - Loss: 0.6932 [352/820]\n",
            "Batch 16/26 - Loss: 0.7017 [512/820]\n",
            "Batch 21/26 - Loss: 0.6987 [672/820]\n",
            "Batch 26/26 - Loss: 0.6945 [520/820]\n",
            "Epoch Training Loss: 0.6981\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 4\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6936 [32/820]\n",
            "Batch 6/26 - Loss: 0.7027 [192/820]\n",
            "Batch 11/26 - Loss: 0.6921 [352/820]\n",
            "Batch 16/26 - Loss: 0.6945 [512/820]\n",
            "Batch 21/26 - Loss: 0.6976 [672/820]\n",
            "Batch 26/26 - Loss: 0.6919 [520/820]\n",
            "Epoch Training Loss: 0.6981\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 5\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6966 [32/820]\n",
            "Batch 6/26 - Loss: 0.6978 [192/820]\n",
            "Batch 11/26 - Loss: 0.6978 [352/820]\n",
            "Batch 16/26 - Loss: 0.6991 [512/820]\n",
            "Batch 21/26 - Loss: 0.6952 [672/820]\n",
            "Batch 26/26 - Loss: 0.7002 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 6\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6955 [32/820]\n",
            "Batch 6/26 - Loss: 0.6998 [192/820]\n",
            "Batch 11/26 - Loss: 0.6944 [352/820]\n",
            "Batch 16/26 - Loss: 0.7004 [512/820]\n",
            "Batch 21/26 - Loss: 0.6975 [672/820]\n",
            "Batch 26/26 - Loss: 0.6926 [520/820]\n",
            "Epoch Training Loss: 0.6981\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 7\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6985 [32/820]\n",
            "Batch 6/26 - Loss: 0.7012 [192/820]\n",
            "Batch 11/26 - Loss: 0.6954 [352/820]\n",
            "Batch 16/26 - Loss: 0.6962 [512/820]\n",
            "Batch 21/26 - Loss: 0.7004 [672/820]\n",
            "Batch 26/26 - Loss: 0.6958 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 8\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6973 [32/820]\n",
            "Batch 6/26 - Loss: 0.7022 [192/820]\n",
            "Batch 11/26 - Loss: 0.7006 [352/820]\n",
            "Batch 16/26 - Loss: 0.6996 [512/820]\n",
            "Batch 21/26 - Loss: 0.7009 [672/820]\n",
            "Batch 26/26 - Loss: 0.6972 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 9\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7013 [32/820]\n",
            "Batch 6/26 - Loss: 0.6987 [192/820]\n",
            "Batch 11/26 - Loss: 0.6987 [352/820]\n",
            "Batch 16/26 - Loss: 0.6931 [512/820]\n",
            "Batch 21/26 - Loss: 0.6956 [672/820]\n",
            "Batch 26/26 - Loss: 0.6997 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 10\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6963 [32/820]\n",
            "Batch 6/26 - Loss: 0.7014 [192/820]\n",
            "Batch 11/26 - Loss: 0.6957 [352/820]\n",
            "Batch 16/26 - Loss: 0.6979 [512/820]\n",
            "Batch 21/26 - Loss: 0.7034 [672/820]\n",
            "Batch 26/26 - Loss: 0.6954 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 11\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7033 [32/820]\n",
            "Batch 6/26 - Loss: 0.7024 [192/820]\n",
            "Batch 11/26 - Loss: 0.6941 [352/820]\n",
            "Batch 16/26 - Loss: 0.7024 [512/820]\n",
            "Batch 21/26 - Loss: 0.6967 [672/820]\n",
            "Batch 26/26 - Loss: 0.6958 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 12\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6988 [32/820]\n",
            "Batch 6/26 - Loss: 0.6984 [192/820]\n",
            "Batch 11/26 - Loss: 0.6973 [352/820]\n",
            "Batch 16/26 - Loss: 0.6951 [512/820]\n",
            "Batch 21/26 - Loss: 0.7030 [672/820]\n",
            "Batch 26/26 - Loss: 0.7025 [520/820]\n",
            "Epoch Training Loss: 0.6983\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 13\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7018 [32/820]\n",
            "Batch 6/26 - Loss: 0.7016 [192/820]\n",
            "Batch 11/26 - Loss: 0.7003 [352/820]\n",
            "Batch 16/26 - Loss: 0.6989 [512/820]\n",
            "Batch 21/26 - Loss: 0.6944 [672/820]\n",
            "Batch 26/26 - Loss: 0.7071 [520/820]\n",
            "Epoch Training Loss: 0.6983\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 14\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7011 [32/820]\n",
            "Batch 6/26 - Loss: 0.6992 [192/820]\n",
            "Batch 11/26 - Loss: 0.7003 [352/820]\n",
            "Batch 16/26 - Loss: 0.6970 [512/820]\n",
            "Batch 21/26 - Loss: 0.6937 [672/820]\n",
            "Batch 26/26 - Loss: 0.6996 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 15\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6986 [32/820]\n",
            "Batch 6/26 - Loss: 0.7005 [192/820]\n",
            "Batch 11/26 - Loss: 0.6951 [352/820]\n",
            "Batch 16/26 - Loss: 0.7017 [512/820]\n",
            "Batch 21/26 - Loss: 0.6945 [672/820]\n",
            "Batch 26/26 - Loss: 0.6913 [520/820]\n",
            "Epoch Training Loss: 0.6981\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 16\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6968 [32/820]\n",
            "Batch 6/26 - Loss: 0.6962 [192/820]\n",
            "Batch 11/26 - Loss: 0.7009 [352/820]\n",
            "Batch 16/26 - Loss: 0.6968 [512/820]\n",
            "Batch 21/26 - Loss: 0.6936 [672/820]\n",
            "Batch 26/26 - Loss: 0.6981 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 17\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6974 [32/820]\n",
            "Batch 6/26 - Loss: 0.6926 [192/820]\n",
            "Batch 11/26 - Loss: 0.6956 [352/820]\n",
            "Batch 16/26 - Loss: 0.6973 [512/820]\n",
            "Batch 21/26 - Loss: 0.6970 [672/820]\n",
            "Batch 26/26 - Loss: 0.7015 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 18\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6981 [32/820]\n",
            "Batch 6/26 - Loss: 0.7079 [192/820]\n",
            "Batch 11/26 - Loss: 0.6966 [352/820]\n",
            "Batch 16/26 - Loss: 0.6994 [512/820]\n",
            "Batch 21/26 - Loss: 0.6942 [672/820]\n",
            "Batch 26/26 - Loss: 0.6986 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 19\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7018 [32/820]\n",
            "Batch 6/26 - Loss: 0.6970 [192/820]\n",
            "Batch 11/26 - Loss: 0.6962 [352/820]\n",
            "Batch 16/26 - Loss: 0.6970 [512/820]\n",
            "Batch 21/26 - Loss: 0.7030 [672/820]\n",
            "Batch 26/26 - Loss: 0.7005 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 20\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6946 [32/820]\n",
            "Batch 6/26 - Loss: 0.7021 [192/820]\n",
            "Batch 11/26 - Loss: 0.6987 [352/820]\n",
            "Batch 16/26 - Loss: 0.6994 [512/820]\n",
            "Batch 21/26 - Loss: 0.6986 [672/820]\n",
            "Batch 26/26 - Loss: 0.6948 [520/820]\n",
            "Epoch Training Loss: 0.6981\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 21\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6969 [32/820]\n",
            "Batch 6/26 - Loss: 0.6961 [192/820]\n",
            "Batch 11/26 - Loss: 0.6997 [352/820]\n",
            "Batch 16/26 - Loss: 0.7021 [512/820]\n",
            "Batch 21/26 - Loss: 0.6980 [672/820]\n",
            "Batch 26/26 - Loss: 0.6977 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 22\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6999 [32/820]\n",
            "Batch 6/26 - Loss: 0.6901 [192/820]\n",
            "Batch 11/26 - Loss: 0.7003 [352/820]\n",
            "Batch 16/26 - Loss: 0.6982 [512/820]\n",
            "Batch 21/26 - Loss: 0.6957 [672/820]\n",
            "Batch 26/26 - Loss: 0.6998 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 23\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6946 [32/820]\n",
            "Batch 6/26 - Loss: 0.6959 [192/820]\n",
            "Batch 11/26 - Loss: 0.6994 [352/820]\n",
            "Batch 16/26 - Loss: 0.6958 [512/820]\n",
            "Batch 21/26 - Loss: 0.7002 [672/820]\n",
            "Batch 26/26 - Loss: 0.6923 [520/820]\n",
            "Epoch Training Loss: 0.6981\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 24\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7028 [32/820]\n",
            "Batch 6/26 - Loss: 0.6946 [192/820]\n",
            "Batch 11/26 - Loss: 0.6921 [352/820]\n",
            "Batch 16/26 - Loss: 0.6998 [512/820]\n",
            "Batch 21/26 - Loss: 0.7013 [672/820]\n",
            "Batch 26/26 - Loss: 0.6980 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 25\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6972 [32/820]\n",
            "Batch 6/26 - Loss: 0.6942 [192/820]\n",
            "Batch 11/26 - Loss: 0.6968 [352/820]\n",
            "Batch 16/26 - Loss: 0.6998 [512/820]\n",
            "Batch 21/26 - Loss: 0.6990 [672/820]\n",
            "Batch 26/26 - Loss: 0.7016 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 26\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6948 [32/820]\n",
            "Batch 6/26 - Loss: 0.6986 [192/820]\n",
            "Batch 11/26 - Loss: 0.6923 [352/820]\n",
            "Batch 16/26 - Loss: 0.7057 [512/820]\n",
            "Batch 21/26 - Loss: 0.6950 [672/820]\n",
            "Batch 26/26 - Loss: 0.6914 [520/820]\n",
            "Epoch Training Loss: 0.6981\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 27\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6972 [32/820]\n",
            "Batch 6/26 - Loss: 0.7007 [192/820]\n",
            "Batch 11/26 - Loss: 0.7011 [352/820]\n",
            "Batch 16/26 - Loss: 0.6934 [512/820]\n",
            "Batch 21/26 - Loss: 0.7027 [672/820]\n",
            "Batch 26/26 - Loss: 0.7091 [520/820]\n",
            "Epoch Training Loss: 0.6983\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 28\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6964 [32/820]\n",
            "Batch 6/26 - Loss: 0.6981 [192/820]\n",
            "Batch 11/26 - Loss: 0.6981 [352/820]\n",
            "Batch 16/26 - Loss: 0.6981 [512/820]\n",
            "Batch 21/26 - Loss: 0.6968 [672/820]\n",
            "Batch 26/26 - Loss: 0.7034 [520/820]\n",
            "Epoch Training Loss: 0.6983\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 29\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6969 [32/820]\n",
            "Batch 6/26 - Loss: 0.7001 [192/820]\n",
            "Batch 11/26 - Loss: 0.7030 [352/820]\n",
            "Batch 16/26 - Loss: 0.6946 [512/820]\n",
            "Batch 21/26 - Loss: 0.6974 [672/820]\n",
            "Batch 26/26 - Loss: 0.6922 [520/820]\n",
            "Epoch Training Loss: 0.6981\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n",
            "Epoch 30\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7032 [32/820]\n",
            "Batch 6/26 - Loss: 0.6964 [192/820]\n",
            "Batch 11/26 - Loss: 0.6961 [352/820]\n",
            "Batch 16/26 - Loss: 0.6987 [512/820]\n",
            "Batch 21/26 - Loss: 0.6960 [672/820]\n",
            "Batch 26/26 - Loss: 0.6995 [520/820]\n",
            "Epoch Training Loss: 0.6982\n",
            "Test Results:\n",
            " Accuracy: 45.4%, Avg Loss: 0.6969\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20241122_044045-u8jaqks4/files/NeuralNetworkTanhSigmoid.pth']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the 2nd Neural Network:"
      ],
      "metadata": {
        "id": "I78ZNIOuGRZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetworkSigmoid().to(device)\n",
        "optimizer = torch.optim.Adam(model_sigmoid.parameters(), lr=wandb.config.learning_rate)\n",
        "\n",
        "for epoch in range(wandb.config.epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------------------\")\n",
        "    train(train_loader, model, loss_func, optimizer)\n",
        "    test(test_loader, model, loss_func)\n",
        "\n",
        "torch.save(model.state_dict(), \"NeuralNetworkSigmoid.pth\")\n",
        "wandb.save(\"NeuralNetworkSigmoid.pth\")"
      ],
      "metadata": {
        "id": "MhqeP8Ik0VOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5077f4-300e-48f7-dde2-4d52da77bfeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7138 [32/820]\n",
            "Batch 6/26 - Loss: 0.6915 [192/820]\n",
            "Batch 11/26 - Loss: 0.6915 [352/820]\n",
            "Batch 16/26 - Loss: 0.6915 [512/820]\n",
            "Batch 21/26 - Loss: 0.6915 [672/820]\n",
            "Batch 26/26 - Loss: 0.6938 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 2\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6938 [32/820]\n",
            "Batch 6/26 - Loss: 0.6960 [192/820]\n",
            "Batch 11/26 - Loss: 0.6982 [352/820]\n",
            "Batch 16/26 - Loss: 0.6982 [512/820]\n",
            "Batch 21/26 - Loss: 0.6938 [672/820]\n",
            "Batch 26/26 - Loss: 0.6867 [520/820]\n",
            "Epoch Training Loss: 0.6948\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 3\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6916 [32/820]\n",
            "Batch 6/26 - Loss: 0.7049 [192/820]\n",
            "Batch 11/26 - Loss: 0.7071 [352/820]\n",
            "Batch 16/26 - Loss: 0.6916 [512/820]\n",
            "Batch 21/26 - Loss: 0.6982 [672/820]\n",
            "Batch 26/26 - Loss: 0.6973 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 4\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6760 [32/820]\n",
            "Batch 6/26 - Loss: 0.7027 [192/820]\n",
            "Batch 11/26 - Loss: 0.6871 [352/820]\n",
            "Batch 16/26 - Loss: 0.7049 [512/820]\n",
            "Batch 21/26 - Loss: 0.6960 [672/820]\n",
            "Batch 26/26 - Loss: 0.6973 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 5\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7004 [32/820]\n",
            "Batch 6/26 - Loss: 0.6982 [192/820]\n",
            "Batch 11/26 - Loss: 0.6960 [352/820]\n",
            "Batch 16/26 - Loss: 0.6960 [512/820]\n",
            "Batch 21/26 - Loss: 0.6982 [672/820]\n",
            "Batch 26/26 - Loss: 0.6902 [520/820]\n",
            "Epoch Training Loss: 0.6948\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 6\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6982 [32/820]\n",
            "Batch 6/26 - Loss: 0.6893 [192/820]\n",
            "Batch 11/26 - Loss: 0.6982 [352/820]\n",
            "Batch 16/26 - Loss: 0.6938 [512/820]\n",
            "Batch 21/26 - Loss: 0.6804 [672/820]\n",
            "Batch 26/26 - Loss: 0.7009 [520/820]\n",
            "Epoch Training Loss: 0.6950\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 7\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6960 [32/820]\n",
            "Batch 6/26 - Loss: 0.6982 [192/820]\n",
            "Batch 11/26 - Loss: 0.6938 [352/820]\n",
            "Batch 16/26 - Loss: 0.6849 [512/820]\n",
            "Batch 21/26 - Loss: 0.6938 [672/820]\n",
            "Batch 26/26 - Loss: 0.7080 [520/820]\n",
            "Epoch Training Loss: 0.6951\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 8\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7027 [32/820]\n",
            "Batch 6/26 - Loss: 0.6982 [192/820]\n",
            "Batch 11/26 - Loss: 0.7004 [352/820]\n",
            "Batch 16/26 - Loss: 0.7004 [512/820]\n",
            "Batch 21/26 - Loss: 0.6938 [672/820]\n",
            "Batch 26/26 - Loss: 0.6831 [520/820]\n",
            "Epoch Training Loss: 0.6947\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 9\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6871 [32/820]\n",
            "Batch 6/26 - Loss: 0.6849 [192/820]\n",
            "Batch 11/26 - Loss: 0.6982 [352/820]\n",
            "Batch 16/26 - Loss: 0.7093 [512/820]\n",
            "Batch 21/26 - Loss: 0.6982 [672/820]\n",
            "Batch 26/26 - Loss: 0.6867 [520/820]\n",
            "Epoch Training Loss: 0.6948\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 10\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6960 [32/820]\n",
            "Batch 6/26 - Loss: 0.6982 [192/820]\n",
            "Batch 11/26 - Loss: 0.6960 [352/820]\n",
            "Batch 16/26 - Loss: 0.6938 [512/820]\n",
            "Batch 21/26 - Loss: 0.6893 [672/820]\n",
            "Batch 26/26 - Loss: 0.6902 [520/820]\n",
            "Epoch Training Loss: 0.6948\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 11\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6938 [32/820]\n",
            "Batch 6/26 - Loss: 0.6938 [192/820]\n",
            "Batch 11/26 - Loss: 0.6938 [352/820]\n",
            "Batch 16/26 - Loss: 0.7049 [512/820]\n",
            "Batch 21/26 - Loss: 0.6916 [672/820]\n",
            "Batch 26/26 - Loss: 0.7009 [520/820]\n",
            "Epoch Training Loss: 0.6950\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 12\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6960 [32/820]\n",
            "Batch 6/26 - Loss: 0.7004 [192/820]\n",
            "Batch 11/26 - Loss: 0.6849 [352/820]\n",
            "Batch 16/26 - Loss: 0.6960 [512/820]\n",
            "Batch 21/26 - Loss: 0.6960 [672/820]\n",
            "Batch 26/26 - Loss: 0.6938 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 13\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6871 [32/820]\n",
            "Batch 6/26 - Loss: 0.6938 [192/820]\n",
            "Batch 11/26 - Loss: 0.6960 [352/820]\n",
            "Batch 16/26 - Loss: 0.6916 [512/820]\n",
            "Batch 21/26 - Loss: 0.6893 [672/820]\n",
            "Batch 26/26 - Loss: 0.6973 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 14\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6960 [32/820]\n",
            "Batch 6/26 - Loss: 0.6760 [192/820]\n",
            "Batch 11/26 - Loss: 0.7004 [352/820]\n",
            "Batch 16/26 - Loss: 0.6915 [512/820]\n",
            "Batch 21/26 - Loss: 0.7004 [672/820]\n",
            "Batch 26/26 - Loss: 0.6867 [520/820]\n",
            "Epoch Training Loss: 0.6948\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 15\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6871 [32/820]\n",
            "Batch 6/26 - Loss: 0.7004 [192/820]\n",
            "Batch 11/26 - Loss: 0.6893 [352/820]\n",
            "Batch 16/26 - Loss: 0.6893 [512/820]\n",
            "Batch 21/26 - Loss: 0.6960 [672/820]\n",
            "Batch 26/26 - Loss: 0.6902 [520/820]\n",
            "Epoch Training Loss: 0.6948\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 16\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6938 [32/820]\n",
            "Batch 6/26 - Loss: 0.6938 [192/820]\n",
            "Batch 11/26 - Loss: 0.6960 [352/820]\n",
            "Batch 16/26 - Loss: 0.6982 [512/820]\n",
            "Batch 21/26 - Loss: 0.7004 [672/820]\n",
            "Batch 26/26 - Loss: 0.7009 [520/820]\n",
            "Epoch Training Loss: 0.6950\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 17\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6982 [32/820]\n",
            "Batch 6/26 - Loss: 0.6871 [192/820]\n",
            "Batch 11/26 - Loss: 0.7004 [352/820]\n",
            "Batch 16/26 - Loss: 0.6960 [512/820]\n",
            "Batch 21/26 - Loss: 0.6893 [672/820]\n",
            "Batch 26/26 - Loss: 0.6831 [520/820]\n",
            "Epoch Training Loss: 0.6947\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 18\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7071 [32/820]\n",
            "Batch 6/26 - Loss: 0.7004 [192/820]\n",
            "Batch 11/26 - Loss: 0.6915 [352/820]\n",
            "Batch 16/26 - Loss: 0.6938 [512/820]\n",
            "Batch 21/26 - Loss: 0.6849 [672/820]\n",
            "Batch 26/26 - Loss: 0.6938 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 19\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6915 [32/820]\n",
            "Batch 6/26 - Loss: 0.6982 [192/820]\n",
            "Batch 11/26 - Loss: 0.6960 [352/820]\n",
            "Batch 16/26 - Loss: 0.7027 [512/820]\n",
            "Batch 21/26 - Loss: 0.6893 [672/820]\n",
            "Batch 26/26 - Loss: 0.7009 [520/820]\n",
            "Epoch Training Loss: 0.6950\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 20\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6960 [32/820]\n",
            "Batch 6/26 - Loss: 0.6915 [192/820]\n",
            "Batch 11/26 - Loss: 0.6893 [352/820]\n",
            "Batch 16/26 - Loss: 0.6938 [512/820]\n",
            "Batch 21/26 - Loss: 0.6916 [672/820]\n",
            "Batch 26/26 - Loss: 0.6902 [520/820]\n",
            "Epoch Training Loss: 0.6948\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 21\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6916 [32/820]\n",
            "Batch 6/26 - Loss: 0.6893 [192/820]\n",
            "Batch 11/26 - Loss: 0.6982 [352/820]\n",
            "Batch 16/26 - Loss: 0.6960 [512/820]\n",
            "Batch 21/26 - Loss: 0.7027 [672/820]\n",
            "Batch 26/26 - Loss: 0.6973 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 22\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7004 [32/820]\n",
            "Batch 6/26 - Loss: 0.6982 [192/820]\n",
            "Batch 11/26 - Loss: 0.7004 [352/820]\n",
            "Batch 16/26 - Loss: 0.6916 [512/820]\n",
            "Batch 21/26 - Loss: 0.6893 [672/820]\n",
            "Batch 26/26 - Loss: 0.6938 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 23\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6982 [32/820]\n",
            "Batch 6/26 - Loss: 0.6916 [192/820]\n",
            "Batch 11/26 - Loss: 0.6960 [352/820]\n",
            "Batch 16/26 - Loss: 0.7027 [512/820]\n",
            "Batch 21/26 - Loss: 0.7004 [672/820]\n",
            "Batch 26/26 - Loss: 0.6902 [520/820]\n",
            "Epoch Training Loss: 0.6948\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 24\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6871 [32/820]\n",
            "Batch 6/26 - Loss: 0.6916 [192/820]\n",
            "Batch 11/26 - Loss: 0.6982 [352/820]\n",
            "Batch 16/26 - Loss: 0.6960 [512/820]\n",
            "Batch 21/26 - Loss: 0.7004 [672/820]\n",
            "Batch 26/26 - Loss: 0.6938 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 25\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6960 [32/820]\n",
            "Batch 6/26 - Loss: 0.6938 [192/820]\n",
            "Batch 11/26 - Loss: 0.6982 [352/820]\n",
            "Batch 16/26 - Loss: 0.7004 [512/820]\n",
            "Batch 21/26 - Loss: 0.6982 [672/820]\n",
            "Batch 26/26 - Loss: 0.6973 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 26\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7004 [32/820]\n",
            "Batch 6/26 - Loss: 0.7027 [192/820]\n",
            "Batch 11/26 - Loss: 0.6916 [352/820]\n",
            "Batch 16/26 - Loss: 0.6893 [512/820]\n",
            "Batch 21/26 - Loss: 0.6938 [672/820]\n",
            "Batch 26/26 - Loss: 0.6831 [520/820]\n",
            "Epoch Training Loss: 0.6947\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 27\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6938 [32/820]\n",
            "Batch 6/26 - Loss: 0.6938 [192/820]\n",
            "Batch 11/26 - Loss: 0.6982 [352/820]\n",
            "Batch 16/26 - Loss: 0.6871 [512/820]\n",
            "Batch 21/26 - Loss: 0.6960 [672/820]\n",
            "Batch 26/26 - Loss: 0.7044 [520/820]\n",
            "Epoch Training Loss: 0.6950\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 28\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6982 [32/820]\n",
            "Batch 6/26 - Loss: 0.6916 [192/820]\n",
            "Batch 11/26 - Loss: 0.6871 [352/820]\n",
            "Batch 16/26 - Loss: 0.7004 [512/820]\n",
            "Batch 21/26 - Loss: 0.7027 [672/820]\n",
            "Batch 26/26 - Loss: 0.6938 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 29\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.7027 [32/820]\n",
            "Batch 6/26 - Loss: 0.7049 [192/820]\n",
            "Batch 11/26 - Loss: 0.6893 [352/820]\n",
            "Batch 16/26 - Loss: 0.6982 [512/820]\n",
            "Batch 21/26 - Loss: 0.6982 [672/820]\n",
            "Batch 26/26 - Loss: 0.6973 [520/820]\n",
            "Epoch Training Loss: 0.6949\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n",
            "Epoch 30\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6960 [32/820]\n",
            "Batch 6/26 - Loss: 0.7004 [192/820]\n",
            "Batch 11/26 - Loss: 0.6938 [352/820]\n",
            "Batch 16/26 - Loss: 0.7027 [512/820]\n",
            "Batch 21/26 - Loss: 0.7004 [672/820]\n",
            "Batch 26/26 - Loss: 0.7009 [520/820]\n",
            "Epoch Training Loss: 0.6950\n",
            "Test Results:\n",
            " Accuracy: 49.8%, Avg Loss: 0.6937\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20241122_044045-u8jaqks4/files/NeuralNetworkSigmoid.pth']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the 3rd Neural Network:"
      ],
      "metadata": {
        "id": "CHoVlip4GVTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetworkReLUSigmoid().to(device)\n",
        "optimizer = torch.optim.Adam(model_relu_sigmoid.parameters(), lr=wandb.config.learning_rate)\n",
        "\n",
        "for epoch in range(wandb.config.epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n------------------------------\")\n",
        "    train(train_loader, model, loss_func, optimizer)\n",
        "    test(test_loader, model, loss_func)\n",
        "\n",
        "torch.save(model.state_dict(), \"NeuralNetworkReLUSigmoid.pth\")\n",
        "wandb.save(\"NeuralNetworkReLUSigmoid.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq4GcuyQBNz-",
        "outputId": "558c2394-70b7-47fb-8fe8-fb858156184c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6923 [32/820]\n",
            "Batch 6/26 - Loss: 0.6927 [192/820]\n",
            "Batch 11/26 - Loss: 0.6908 [352/820]\n",
            "Batch 16/26 - Loss: 0.6938 [512/820]\n",
            "Batch 21/26 - Loss: 0.6933 [672/820]\n",
            "Batch 26/26 - Loss: 0.6930 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 2\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6911 [32/820]\n",
            "Batch 6/26 - Loss: 0.6909 [192/820]\n",
            "Batch 11/26 - Loss: 0.6920 [352/820]\n",
            "Batch 16/26 - Loss: 0.6925 [512/820]\n",
            "Batch 21/26 - Loss: 0.6913 [672/820]\n",
            "Batch 26/26 - Loss: 0.6938 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 3\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6944 [32/820]\n",
            "Batch 6/26 - Loss: 0.6933 [192/820]\n",
            "Batch 11/26 - Loss: 0.6932 [352/820]\n",
            "Batch 16/26 - Loss: 0.6932 [512/820]\n",
            "Batch 21/26 - Loss: 0.6936 [672/820]\n",
            "Batch 26/26 - Loss: 0.6916 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 4\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6912 [32/820]\n",
            "Batch 6/26 - Loss: 0.6929 [192/820]\n",
            "Batch 11/26 - Loss: 0.6935 [352/820]\n",
            "Batch 16/26 - Loss: 0.6949 [512/820]\n",
            "Batch 21/26 - Loss: 0.6942 [672/820]\n",
            "Batch 26/26 - Loss: 0.6924 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 5\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6943 [32/820]\n",
            "Batch 6/26 - Loss: 0.6915 [192/820]\n",
            "Batch 11/26 - Loss: 0.6945 [352/820]\n",
            "Batch 16/26 - Loss: 0.6935 [512/820]\n",
            "Batch 21/26 - Loss: 0.6941 [672/820]\n",
            "Batch 26/26 - Loss: 0.6923 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 6\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6915 [32/820]\n",
            "Batch 6/26 - Loss: 0.6923 [192/820]\n",
            "Batch 11/26 - Loss: 0.6922 [352/820]\n",
            "Batch 16/26 - Loss: 0.6941 [512/820]\n",
            "Batch 21/26 - Loss: 0.6916 [672/820]\n",
            "Batch 26/26 - Loss: 0.6908 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 7\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6906 [32/820]\n",
            "Batch 6/26 - Loss: 0.6941 [192/820]\n",
            "Batch 11/26 - Loss: 0.6929 [352/820]\n",
            "Batch 16/26 - Loss: 0.6924 [512/820]\n",
            "Batch 21/26 - Loss: 0.6933 [672/820]\n",
            "Batch 26/26 - Loss: 0.6933 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 8\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6954 [32/820]\n",
            "Batch 6/26 - Loss: 0.6917 [192/820]\n",
            "Batch 11/26 - Loss: 0.6918 [352/820]\n",
            "Batch 16/26 - Loss: 0.6925 [512/820]\n",
            "Batch 21/26 - Loss: 0.6939 [672/820]\n",
            "Batch 26/26 - Loss: 0.6903 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 9\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6930 [32/820]\n",
            "Batch 6/26 - Loss: 0.6921 [192/820]\n",
            "Batch 11/26 - Loss: 0.6902 [352/820]\n",
            "Batch 16/26 - Loss: 0.6939 [512/820]\n",
            "Batch 21/26 - Loss: 0.6915 [672/820]\n",
            "Batch 26/26 - Loss: 0.6922 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 10\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6925 [32/820]\n",
            "Batch 6/26 - Loss: 0.6938 [192/820]\n",
            "Batch 11/26 - Loss: 0.6933 [352/820]\n",
            "Batch 16/26 - Loss: 0.6918 [512/820]\n",
            "Batch 21/26 - Loss: 0.6926 [672/820]\n",
            "Batch 26/26 - Loss: 0.6949 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 11\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6935 [32/820]\n",
            "Batch 6/26 - Loss: 0.6928 [192/820]\n",
            "Batch 11/26 - Loss: 0.6924 [352/820]\n",
            "Batch 16/26 - Loss: 0.6911 [512/820]\n",
            "Batch 21/26 - Loss: 0.6921 [672/820]\n",
            "Batch 26/26 - Loss: 0.6915 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 12\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6921 [32/820]\n",
            "Batch 6/26 - Loss: 0.6943 [192/820]\n",
            "Batch 11/26 - Loss: 0.6924 [352/820]\n",
            "Batch 16/26 - Loss: 0.6910 [512/820]\n",
            "Batch 21/26 - Loss: 0.6940 [672/820]\n",
            "Batch 26/26 - Loss: 0.6944 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 13\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6914 [32/820]\n",
            "Batch 6/26 - Loss: 0.6939 [192/820]\n",
            "Batch 11/26 - Loss: 0.6918 [352/820]\n",
            "Batch 16/26 - Loss: 0.6947 [512/820]\n",
            "Batch 21/26 - Loss: 0.6950 [672/820]\n",
            "Batch 26/26 - Loss: 0.6933 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 14\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6930 [32/820]\n",
            "Batch 6/26 - Loss: 0.6914 [192/820]\n",
            "Batch 11/26 - Loss: 0.6941 [352/820]\n",
            "Batch 16/26 - Loss: 0.6958 [512/820]\n",
            "Batch 21/26 - Loss: 0.6924 [672/820]\n",
            "Batch 26/26 - Loss: 0.6892 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 15\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6910 [32/820]\n",
            "Batch 6/26 - Loss: 0.6932 [192/820]\n",
            "Batch 11/26 - Loss: 0.6924 [352/820]\n",
            "Batch 16/26 - Loss: 0.6942 [512/820]\n",
            "Batch 21/26 - Loss: 0.6916 [672/820]\n",
            "Batch 26/26 - Loss: 0.6902 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 16\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6925 [32/820]\n",
            "Batch 6/26 - Loss: 0.6927 [192/820]\n",
            "Batch 11/26 - Loss: 0.6928 [352/820]\n",
            "Batch 16/26 - Loss: 0.6942 [512/820]\n",
            "Batch 21/26 - Loss: 0.6930 [672/820]\n",
            "Batch 26/26 - Loss: 0.6927 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 17\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6924 [32/820]\n",
            "Batch 6/26 - Loss: 0.6927 [192/820]\n",
            "Batch 11/26 - Loss: 0.6935 [352/820]\n",
            "Batch 16/26 - Loss: 0.6943 [512/820]\n",
            "Batch 21/26 - Loss: 0.6916 [672/820]\n",
            "Batch 26/26 - Loss: 0.6900 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 18\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6906 [32/820]\n",
            "Batch 6/26 - Loss: 0.6917 [192/820]\n",
            "Batch 11/26 - Loss: 0.6939 [352/820]\n",
            "Batch 16/26 - Loss: 0.6938 [512/820]\n",
            "Batch 21/26 - Loss: 0.6921 [672/820]\n",
            "Batch 26/26 - Loss: 0.6923 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 19\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6920 [32/820]\n",
            "Batch 6/26 - Loss: 0.6923 [192/820]\n",
            "Batch 11/26 - Loss: 0.6918 [352/820]\n",
            "Batch 16/26 - Loss: 0.6917 [512/820]\n",
            "Batch 21/26 - Loss: 0.6926 [672/820]\n",
            "Batch 26/26 - Loss: 0.6944 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 20\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6927 [32/820]\n",
            "Batch 6/26 - Loss: 0.6917 [192/820]\n",
            "Batch 11/26 - Loss: 0.6907 [352/820]\n",
            "Batch 16/26 - Loss: 0.6939 [512/820]\n",
            "Batch 21/26 - Loss: 0.6915 [672/820]\n",
            "Batch 26/26 - Loss: 0.6941 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 21\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6895 [32/820]\n",
            "Batch 6/26 - Loss: 0.6923 [192/820]\n",
            "Batch 11/26 - Loss: 0.6924 [352/820]\n",
            "Batch 16/26 - Loss: 0.6933 [512/820]\n",
            "Batch 21/26 - Loss: 0.6951 [672/820]\n",
            "Batch 26/26 - Loss: 0.6923 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 22\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6939 [32/820]\n",
            "Batch 6/26 - Loss: 0.6926 [192/820]\n",
            "Batch 11/26 - Loss: 0.6934 [352/820]\n",
            "Batch 16/26 - Loss: 0.6934 [512/820]\n",
            "Batch 21/26 - Loss: 0.6923 [672/820]\n",
            "Batch 26/26 - Loss: 0.6950 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 23\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6914 [32/820]\n",
            "Batch 6/26 - Loss: 0.6934 [192/820]\n",
            "Batch 11/26 - Loss: 0.6918 [352/820]\n",
            "Batch 16/26 - Loss: 0.6934 [512/820]\n",
            "Batch 21/26 - Loss: 0.6909 [672/820]\n",
            "Batch 26/26 - Loss: 0.6937 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 24\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6934 [32/820]\n",
            "Batch 6/26 - Loss: 0.6913 [192/820]\n",
            "Batch 11/26 - Loss: 0.6923 [352/820]\n",
            "Batch 16/26 - Loss: 0.6949 [512/820]\n",
            "Batch 21/26 - Loss: 0.6922 [672/820]\n",
            "Batch 26/26 - Loss: 0.6929 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 25\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6922 [32/820]\n",
            "Batch 6/26 - Loss: 0.6930 [192/820]\n",
            "Batch 11/26 - Loss: 0.6946 [352/820]\n",
            "Batch 16/26 - Loss: 0.6937 [512/820]\n",
            "Batch 21/26 - Loss: 0.6921 [672/820]\n",
            "Batch 26/26 - Loss: 0.6893 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 26\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6944 [32/820]\n",
            "Batch 6/26 - Loss: 0.6919 [192/820]\n",
            "Batch 11/26 - Loss: 0.6929 [352/820]\n",
            "Batch 16/26 - Loss: 0.6926 [512/820]\n",
            "Batch 21/26 - Loss: 0.6918 [672/820]\n",
            "Batch 26/26 - Loss: 0.6931 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 27\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6927 [32/820]\n",
            "Batch 6/26 - Loss: 0.6953 [192/820]\n",
            "Batch 11/26 - Loss: 0.6931 [352/820]\n",
            "Batch 16/26 - Loss: 0.6934 [512/820]\n",
            "Batch 21/26 - Loss: 0.6914 [672/820]\n",
            "Batch 26/26 - Loss: 0.6952 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 28\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6910 [32/820]\n",
            "Batch 6/26 - Loss: 0.6936 [192/820]\n",
            "Batch 11/26 - Loss: 0.6929 [352/820]\n",
            "Batch 16/26 - Loss: 0.6933 [512/820]\n",
            "Batch 21/26 - Loss: 0.6931 [672/820]\n",
            "Batch 26/26 - Loss: 0.6932 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 29\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6947 [32/820]\n",
            "Batch 6/26 - Loss: 0.6926 [192/820]\n",
            "Batch 11/26 - Loss: 0.6932 [352/820]\n",
            "Batch 16/26 - Loss: 0.6932 [512/820]\n",
            "Batch 21/26 - Loss: 0.6940 [672/820]\n",
            "Batch 26/26 - Loss: 0.6910 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n",
            "Epoch 30\n",
            "------------------------------\n",
            "Batch 1/26 - Loss: 0.6944 [32/820]\n",
            "Batch 6/26 - Loss: 0.6914 [192/820]\n",
            "Batch 11/26 - Loss: 0.6938 [352/820]\n",
            "Batch 16/26 - Loss: 0.6941 [512/820]\n",
            "Batch 21/26 - Loss: 0.6923 [672/820]\n",
            "Batch 26/26 - Loss: 0.6921 [520/820]\n",
            "Epoch Training Loss: 0.6927\n",
            "Test Results:\n",
            " Accuracy: 50.2%, Avg Loss: 0.6927\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20241122_044045-u8jaqks4/files/NeuralNetworkReLUSigmoid.pth']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**"
      ],
      "metadata": {
        "id": "9ldKy8nCHUu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment revealed that none of the models achieved meaningful learning or significant performance improvements. The Tanh-Sigmoid model, which typically supports effective gradient flow, performed the worst with an accuracy of 45.4%. This suggests potential issues with data quality as the architecture itself should theoretically perform better.\n",
        "\n",
        "The Sigmoid-Sigmoid model achieved an accuracy of 49.8%. Its consistent lack of improvement highlights the limitations of using Sigmoid activations throughout the network, as this likely caused vanishing gradients that hindered effective weight updates.\n",
        "\n",
        "The ReLU-Sigmoid model showed slightly better performance with an accuracy of 50.2%, but it also failed to learn meaningful patterns. While ReLU addresses vanishing gradients in the hidden layers, the combination of ReLU and Sigmoid activations appears to be ineffective for this particular task.\n",
        "\n",
        "Overall, the poor performance across all models suggests that external factors, such as data preprocessing, initialization, or optimization settings, may have contributed to these results."
      ],
      "metadata": {
        "id": "fK37SvdFHYD2"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}